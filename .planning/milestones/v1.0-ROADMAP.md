# Milestone v1.0: MVP

**Status:** SHIPPED 2026-01-22
**Phases:** 1-5
**Total Plans:** 17

## Overview

Build the core RLM system with AST-aware chunking, hybrid retrieval via Qdrant, verification-driven recursion, and GSD integration. Deliver a working system that makes existing /gsd commands dramatically smarter at understanding large codebases.

## Phases

### Phase 1: Core Infrastructure

**Goal**: Build the foundation â€” AST-aware chunking and hybrid retrieval via Qdrant
**Depends on**: Nothing (first phase)
**Requirements**: IDX-01, IDX-02, IDX-03, IDX-04, VEC-01, VEC-02, VEC-03, VEC-04
**Plans**: 5 plans

Plans:
- [x] 01-01: Qdrant client and collection setup
- [x] 01-02: Tree-sitter AST parsing and code chunking
- [x] 01-03: Markdown chunking by headers
- [x] 01-04: Embedding generation with Ollama
- [x] 01-05: Hybrid search with RRF fusion and indexing pipeline

**Key Deliverables:**
- AST-aware code chunking (TypeScript, JavaScript, Python, Go, Rust, Ruby)
- Markdown chunking by headers with semantic paragraphs
- Hybrid retrieval combining dense vectors + BM25 sparse vectors
- RRF fusion for optimal result ranking
- Full indexing pipeline with incremental updates

### Phase 2: RLM Engine Core

**Goal**: Build the recursive reasoning engine with REPL environment
**Depends on**: Phase 1
**Requirements**: RLM-01, RLM-02, RLM-03, RLM-04, RLM-05
**Plans**: 4 plans

Plans:
- [x] 02-01: Engine types and REPL-style state management
- [x] 02-02: RLMEngine with query/recurse methods and tools
- [x] 02-03: Evidence tracker and confidence scoring
- [x] 02-04: RLMDispatcher pipeline orchestration

**Key Deliverables:**
- RLMEngine with 5 tools: peek_context, search_context, get_chunk, sub_query, final_answer
- REPL-style state with variable storage and context inspection
- Evidence tracking linking claims to source chunks
- Confidence scoring from retrieval scores
- Full pipeline: embed -> retrieve -> query -> verify -> recurse

### Phase 3: Verification Loop

**Goal**: Add evidence validation and recursive refinement on failure
**Depends on**: Phase 2
**Requirements**: VER-01, VER-02, VER-03
**Plans**: 3 plans

Plans:
- [x] 03-01: Verification types and NLP claim extraction
- [x] 03-02: Check implementations (typecheck, test, impact)
- [x] 03-03: Verifier class and dispatcher integration

**Key Deliverables:**
- ClaimExtractor using compromise.js for NLP sentence segmentation
- Atomic claim decomposition (Claimify pattern)
- TypeScript type checking via ts-morph
- Programmatic test execution via Vitest Node API
- Impact analysis via reference scanning
- FIRE-style verification with confidence-based recursion
- Infinite loop prevention (same errors, diminishing returns)

### Phase 4: GSD Integration

**Goal**: Wire RLM into existing /gsd commands transparently
**Depends on**: Phase 3
**Requirements**: VER-04
**Plans**: 2 plans

Plans:
- [x] 04-01: Quick retrieval and context formatting
- [x] 04-02: RLM CLI and install integration

**Key Deliverables:**
- quickRetrieve() for fast semantic search (~100-500ms)
- formatChunksAsContext() for readable markdown context
- RLM CLI with index, query, status commands
- Install integration (RLM dist copied during GSD install)
- Graceful degradation when services unavailable

### Phase 5: Optimization & Polish

**Goal**: Hit <500ms latency target, add caching, finalize quality
**Depends on**: Phase 4
**Requirements**: OPT-01, OPT-02, OPT-03, OPT-04, QUA-02, QUA-03
**Plans**: 3 plans

Plans:
- [x] 05-01: Embedding cache layer with LRU eviction
- [x] 05-02: Performance benchmarking suite with Vitest bench
- [x] 05-03: Graceful degradation, latency verification, documentation

**Key Deliverables:**
- LRU embedding cache (10k entries, 500MB limit, 24h TTL)
- Vitest benchmark suite for embedding and retrieval latency
- Qdrant scalar quantization (int8, always_ram=true)
- Graceful degradation (never throw, return empty results)
- CONTRIBUTING.md with full contributor documentation
- Latency verified at ~133ms (target <500ms)

---

## Milestone Summary

**Key Decisions:**

| Decision | Rationale | Outcome |
|----------|-----------|---------|
| Ollama + nomic-embed-text | Free, local, offline-capable | Good |
| Qdrant embedded default | Zero setup for users | Good |
| ES2022 + NodeNext | Modern Node.js compatibility | Good |
| WASM Tree-sitter | Cross-platform, no native compilation | Good |
| RRF fusion | Better ranking than concatenation | Good |
| FIRE-style verification | Principled recursion control | Good |
| lru-cache | 50%+ reduction in Ollama calls | Good |
| Graceful degradation | Better UX than FAISS fallback | Good |
| zod@3.x | v4 not compatible with zod-to-json-schema | Good |

**Issues Resolved:**
- Cross-platform path handling for Windows/Unix
- WASM file deployment via install.js copy
- Qdrant connection resilience with timeout protection
- TypeScript strict mode compliance

**Issues Deferred:**
- 85% test coverage (deferred to v1.1)
- Full /gsd command integration (deferred to v1.1)
- MCP server for Claude Desktop (future milestone)

**Technical Debt Incurred:**
- Test coverage below 85% target
- Some error handling could be more specific
- Benchmark results need CI integration

---

*For current project status, see .planning/ROADMAP.md*
