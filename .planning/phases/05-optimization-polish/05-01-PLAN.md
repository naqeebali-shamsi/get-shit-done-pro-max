---
phase: 05-optimization-polish
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/rlm/cache/embedding-cache.ts, src/rlm/cache/index.ts, src/rlm/embedding/embedder.ts, src/rlm/index.ts]
autonomous: true
---

<objective>
Implement LRU embedding cache layer with content-hash keys to eliminate redundant Ollama embedding calls.

Purpose: Embedding generation is the primary latency bottleneck (200-500ms per call). Caching embeddings by content hash reduces p95 latency from 2+ seconds to <500ms by avoiding redundant computation. This directly addresses OPT-01 and enables OPT-04 latency targets.

Output: Embedding cache module with LRU eviction, TTL expiry, and integration with existing embedder.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-optimization-polish/05-RESEARCH.md
@src/rlm/embedding/embedder.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install lru-cache dependency</name>
  <files>package.json</files>
  <action>Install lru-cache ^10.x for in-memory embedding cache. This is the standard library with best TypeScript support, TTL, and async fetchMethod pattern. Run: npm install lru-cache</action>
  <verify>npm list lru-cache shows ^10.x installed</verify>
  <done>lru-cache dependency added to package.json</done>
</task>

<task type="auto">
  <name>Task 2: Create embedding cache module</name>
  <files>src/rlm/cache/embedding-cache.ts, src/rlm/cache/index.ts</files>
  <action>
Create embedding cache with:
1. Content hash function using crypto.sha256 truncated to 16 chars (64 bits sufficient for cache keys)
2. LRU cache with configurable limits:
   - max: 10000 entries (default)
   - maxSize: 500MB with sizeCalculation (embedding.length * 8 bytes for Float64)
   - ttl: 24 hours (embeddings are deterministic but model updates invalidate)
   - updateAgeOnGet: true (reset TTL on access)
3. getOrEmbed(text: string, embedFn: () => Promise<number[]>): Promise<number[]> function
4. Cache stats: hits, misses, size
5. Export EmbeddingCache class and createEmbeddingCache factory

Use pattern from RESEARCH.md - LRU cache with content-hash keys. Do NOT use fetchMethod pattern (requires passing text separately), use getOrEmbed with callback instead.
  </action>
  <verify>tsc --noEmit passes for cache module</verify>
  <done>EmbeddingCache class exports getOrEmbed, contentHash, and stats methods</done>
</task>

<task type="auto">
  <name>Task 3: Integrate cache with embedder</name>
  <files>src/rlm/embedding/embedder.ts, src/rlm/index.ts</files>
  <action>
Modify embedText and embedChunks to use embedding cache:
1. Add optional cache parameter to embedText(text, options?) with useCache: boolean (default true)
2. Create module-level cache instance (singleton pattern)
3. When useCache=true, use cache.getOrEmbed(text, () => ollamaEmbed(text))
4. When useCache=false, bypass cache (for benchmarking)
5. Export getCacheStats() from embedder for monitoring
6. Re-export cache utilities from src/rlm/index.ts

Do NOT change function signatures - add options parameter with backward-compatible defaults.
  </action>
  <verify>npm run build succeeds</verify>
  <done>embedText uses cache by default, cache stats accessible via getCacheStats()</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] npm run build succeeds without errors
- [ ] tsc --noEmit passes
- [ ] EmbeddingCache class is exported from src/rlm/index.ts
- [ ] embedText function caches results by default
- [ ] Second call to embedText with same text returns cached result (verify via stats)
</verification>

<success_criteria>
- All tasks completed
- All verification checks pass
- No errors or warnings introduced
- Embedding cache reduces redundant Ollama calls
- Cache stats show hits on repeated queries
</success_criteria>

<output>
After completion, create `.planning/phases/05-optimization-polish/05-01-SUMMARY.md`
</output>
